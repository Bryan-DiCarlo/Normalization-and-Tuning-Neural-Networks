{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization and Tuning Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For this lab on initialization and optimization, let's look at a slightly different type of neural network. This time, we will not perform a classification task as we've done before (Santa vs not santa, bank complaint types), but we'll look at a linear regression problem.\n",
    "\n",
    "We can just as well use deep learning networks for linear regression as for a classification problem. Do note that getting regression to work with neural networks is a hard problem because the output is unbounded ($\\hat y$ can technically range from $-\\infty$ to $+\\infty$, and the models are especially prone to exploding gradients. This issue makes a regression exercise the perfect learning case!\n",
    "\n",
    "## Objectives\n",
    "You will be able to:\n",
    "* Build a nueral network using keras\n",
    "* Normalize your data to assist algorithm convergence\n",
    "* Implement and observe the impact of various initialization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras import initializers\n",
    "from keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we'll be working with is data related to facebook posts published during the year of 2014 on the Facebook's page of a renowned cosmetics brand.  It includes 7 features known prior to post publication, and 12 features for evaluating the post impact. What we want to do is make a predictor for the number of \"likes\" for a post, taking into account the 7 features prior to posting.\n",
    "\n",
    "First, let's import the data set and delete any rows with missing data. Afterwards, briefly preview the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(495, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page total likes</th>\n",
       "      <th>Type</th>\n",
       "      <th>Category</th>\n",
       "      <th>Post Month</th>\n",
       "      <th>Post Weekday</th>\n",
       "      <th>Post Hour</th>\n",
       "      <th>Paid</th>\n",
       "      <th>Lifetime Post Total Reach</th>\n",
       "      <th>Lifetime Post Total Impressions</th>\n",
       "      <th>Lifetime Engaged Users</th>\n",
       "      <th>Lifetime Post Consumers</th>\n",
       "      <th>Lifetime Post Consumptions</th>\n",
       "      <th>Lifetime Post Impressions by people who have liked your Page</th>\n",
       "      <th>Lifetime Post reach by people who like your Page</th>\n",
       "      <th>Lifetime People who have liked your Page and engaged with your post</th>\n",
       "      <th>comment</th>\n",
       "      <th>like</th>\n",
       "      <th>share</th>\n",
       "      <th>Total Interactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2752</td>\n",
       "      <td>5091</td>\n",
       "      <td>178</td>\n",
       "      <td>109</td>\n",
       "      <td>159</td>\n",
       "      <td>3078</td>\n",
       "      <td>1640</td>\n",
       "      <td>119</td>\n",
       "      <td>4</td>\n",
       "      <td>79.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139441</td>\n",
       "      <td>Status</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10460</td>\n",
       "      <td>19057</td>\n",
       "      <td>1457</td>\n",
       "      <td>1361</td>\n",
       "      <td>1674</td>\n",
       "      <td>11710</td>\n",
       "      <td>6112</td>\n",
       "      <td>1108</td>\n",
       "      <td>5</td>\n",
       "      <td>130.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2413</td>\n",
       "      <td>4373</td>\n",
       "      <td>177</td>\n",
       "      <td>113</td>\n",
       "      <td>154</td>\n",
       "      <td>2812</td>\n",
       "      <td>1503</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50128</td>\n",
       "      <td>87991</td>\n",
       "      <td>2211</td>\n",
       "      <td>790</td>\n",
       "      <td>1119</td>\n",
       "      <td>61027</td>\n",
       "      <td>32048</td>\n",
       "      <td>1386</td>\n",
       "      <td>58</td>\n",
       "      <td>1572.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>1777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7244</td>\n",
       "      <td>13594</td>\n",
       "      <td>671</td>\n",
       "      <td>410</td>\n",
       "      <td>580</td>\n",
       "      <td>6228</td>\n",
       "      <td>3200</td>\n",
       "      <td>396</td>\n",
       "      <td>19</td>\n",
       "      <td>325.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Page total likes    Type  Category  Post Month  Post Weekday  Post Hour  \\\n",
       "0            139441   Photo         2          12             4          3   \n",
       "1            139441  Status         2          12             3         10   \n",
       "2            139441   Photo         3          12             3          3   \n",
       "3            139441   Photo         2          12             2         10   \n",
       "4            139441   Photo         2          12             2          3   \n",
       "\n",
       "   Paid  Lifetime Post Total Reach  Lifetime Post Total Impressions  \\\n",
       "0   0.0                       2752                             5091   \n",
       "1   0.0                      10460                            19057   \n",
       "2   0.0                       2413                             4373   \n",
       "3   1.0                      50128                            87991   \n",
       "4   0.0                       7244                            13594   \n",
       "\n",
       "   Lifetime Engaged Users  Lifetime Post Consumers  \\\n",
       "0                     178                      109   \n",
       "1                    1457                     1361   \n",
       "2                     177                      113   \n",
       "3                    2211                      790   \n",
       "4                     671                      410   \n",
       "\n",
       "   Lifetime Post Consumptions  \\\n",
       "0                         159   \n",
       "1                        1674   \n",
       "2                         154   \n",
       "3                        1119   \n",
       "4                         580   \n",
       "\n",
       "   Lifetime Post Impressions by people who have liked your Page  \\\n",
       "0                                               3078              \n",
       "1                                              11710              \n",
       "2                                               2812              \n",
       "3                                              61027              \n",
       "4                                               6228              \n",
       "\n",
       "   Lifetime Post reach by people who like your Page  \\\n",
       "0                                              1640   \n",
       "1                                              6112   \n",
       "2                                              1503   \n",
       "3                                             32048   \n",
       "4                                              3200   \n",
       "\n",
       "   Lifetime People who have liked your Page and engaged with your post  \\\n",
       "0                                                119                     \n",
       "1                                               1108                     \n",
       "2                                                132                     \n",
       "3                                               1386                     \n",
       "4                                                396                     \n",
       "\n",
       "   comment    like  share  Total Interactions  \n",
       "0        4    79.0   17.0                 100  \n",
       "1        5   130.0   29.0                 164  \n",
       "2        0    66.0   14.0                  80  \n",
       "3       58  1572.0  147.0                1777  \n",
       "4       19   325.0   49.0                 393  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; load the dataset and drop rows with missing values. Then preview the data.\n",
    "data = pd.read_csv(\"dataset_Facebook.csv\", sep = \";\", header=0)\n",
    "data = data.dropna()\n",
    "print(np.shape(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our input data. We'll use the 7 first columns as our predictors. We'll do the following two things:\n",
    "- Normalize the continuous variables --> you can do this using `np.mean()` and `np.std()`\n",
    "- Make dummy variables of the categorical variables (you can do this by using `pd.get_dummies`)\n",
    "\n",
    "We only count \"Category\" and \"Type\" as categorical variables. Note that you can argue that \"Post month\", \"Post Weekday\" and \"Post Hour\" can also be considered categories, but we'll just treat them as being continuous for now.\n",
    "\n",
    "You'll then use these to define X and Y. \n",
    "\n",
    "To summarize, X will be:\n",
    "* Page total likes\n",
    "* Post Month\n",
    "* Post Weekday\n",
    "* Post Hour\n",
    "* Paid\n",
    "along with dummy variables for:\n",
    "* Type\n",
    "* Category\n",
    "\n",
    "\n",
    "Be sure to normalize your features by subtracting the mean and dividing by the standard deviation.  \n",
    "\n",
    "Finally, y will simply be the \"like\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; define X and y.\n",
    "X0 = data[\"Page total likes\"]\n",
    "X1 = data[\"Type\"]\n",
    "X2 = data[\"Category\"]\n",
    "X3 = data[\"Post Month\"]\n",
    "X4 = data[\"Post Weekday\"]\n",
    "X5 = data[\"Post Hour\"]\n",
    "X6 = data[\"Paid\"]\n",
    "\n",
    "## standardize/categorize\n",
    "X0= (X0-np.mean(X0))/(np.std(X0))\n",
    "dummy_X1= pd.get_dummies(X1)\n",
    "dummy_X2= pd.get_dummies(X2)\n",
    "X3= (X3-np.mean(X3))/(np.std(X3))\n",
    "X4= (X4-np.mean(X4))/(np.std(X4))\n",
    "X5= (X5-np.mean(X5))/(np.std(X5))\n",
    "\n",
    "X = pd.concat([X0, dummy_X1, dummy_X2, X3, X4, X5, X6], axis=1)\n",
    "\n",
    "Y = data[\"like\"]\n",
    "\n",
    "#Note: you get the same result for standardization if you use StandardScaler from sklearn.preprocessing\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#sc = StandardScaler()\n",
    "#X0 = sc.fit_transform(X0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is fairly small. Let's just split the data up in a training set and a validation set!  The next three code blocks are all provided for you; have a quick review but not need to make edits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code provided; defining training and validation sets\n",
    "data_clean = pd.concat([X, Y], axis=1)\n",
    "np.random.seed(123)\n",
    "train, validation = train_test_split(data_clean, test_size=0.2)\n",
    "\n",
    "X_val = validation.iloc[:,0:12]\n",
    "Y_val = validation.iloc[:,12]\n",
    "X_train = train.iloc[:,0:12]\n",
    "Y_train = train.iloc[:,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bryan\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\bryan\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "#Code provided; building an initial model\n",
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"sgd\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code provided; previewing the loss through successive epochs\n",
    "hist.history['loss'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you see what happend? all the values for training and validation loss are \"nan\". There could be several reasons for that, but as we already mentioned there is likely a vanishing or exploding gradient problem. recall that we normalized out inputs. But how about the outputs? Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208     54.0\n",
       "290     23.0\n",
       "286     15.0\n",
       "0       79.0\n",
       "401    329.0\n",
       "Name: like, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, indeed. We didn't normalize them and we should, as they take pretty high values. Let\n",
    "s rerun the model but make sure that the output is normalized as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the output\n",
    "\n",
    "Normalize Y as you did X by subtracting the mean and dividing by the standard deviation. Then, resplit the data into training and validation sets as we demonstrated above, and retrain a new model using your normalized X and Y data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = (data[\"like\"]-np.mean(data[\"like\"]))/(np.std(data[\"like\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; create training and validation sets as before. Use random seed 123.\n",
    "data_clean = pd.concat([X, Y], axis=1)\n",
    "np.random.seed(123)\n",
    "train, validation = train_test_split(data_clean, test_size=0.2)\n",
    "\n",
    "X_val = validation.iloc[:,0:12]\n",
    "Y_val = validation.iloc[:,12]\n",
    "X_train = train.iloc[:,0:12]\n",
    "Y_train = train.iloc[:,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 396 samples, validate on 99 samples\n",
      "Epoch 1/100\n",
      "396/396 [==============================] - 0s 1ms/step - loss: 1.2409 - mean_squared_error: 1.2409 - val_loss: 1.3002 - val_mean_squared_error: 1.3002\n",
      "Epoch 2/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.1755 - mean_squared_error: 1.1755 - val_loss: 1.2266 - val_mean_squared_error: 1.2266\n",
      "Epoch 3/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 1.1352 - mean_squared_error: 1.1352 - val_loss: 1.1786 - val_mean_squared_error: 1.1786\n",
      "Epoch 4/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 1.1084 - mean_squared_error: 1.1084 - val_loss: 1.1428 - val_mean_squared_error: 1.1428\n",
      "Epoch 5/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 1.0903 - mean_squared_error: 1.0903 - val_loss: 1.1170 - val_mean_squared_error: 1.1170\n",
      "Epoch 6/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 1.0722 - mean_squared_error: 1.0722 - val_loss: 1.0957 - val_mean_squared_error: 1.0957\n",
      "Epoch 7/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.0612 - mean_squared_error: 1.0612 - val_loss: 1.0786 - val_mean_squared_error: 1.0786\n",
      "Epoch 8/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.0505 - mean_squared_error: 1.0505 - val_loss: 1.0640 - val_mean_squared_error: 1.0640\n",
      "Epoch 9/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 1.0426 - mean_squared_error: 1.0426 - val_loss: 1.0533 - val_mean_squared_error: 1.0533\n",
      "Epoch 10/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 1.0361 - mean_squared_error: 1.0361 - val_loss: 1.0447 - val_mean_squared_error: 1.0447\n",
      "Epoch 11/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.0305 - mean_squared_error: 1.0305 - val_loss: 1.0350 - val_mean_squared_error: 1.0350\n",
      "Epoch 12/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 1.0248 - mean_squared_error: 1.0248 - val_loss: 1.0285 - val_mean_squared_error: 1.0285\n",
      "Epoch 13/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 1.0208 - mean_squared_error: 1.0208 - val_loss: 1.0208 - val_mean_squared_error: 1.0208\n",
      "Epoch 14/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 1.0173 - mean_squared_error: 1.0173 - val_loss: 1.0128 - val_mean_squared_error: 1.0128\n",
      "Epoch 15/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.0134 - mean_squared_error: 1.0134 - val_loss: 1.0081 - val_mean_squared_error: 1.0081\n",
      "Epoch 16/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "Epoch 17/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 1.0068 - mean_squared_error: 1.0068 - val_loss: 0.9981 - val_mean_squared_error: 0.9981\n",
      "Epoch 18/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.0044 - mean_squared_error: 1.0044 - val_loss: 0.9967 - val_mean_squared_error: 0.9967\n",
      "Epoch 19/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.0019 - mean_squared_error: 1.0019 - val_loss: 0.9932 - val_mean_squared_error: 0.9932\n",
      "Epoch 20/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.0004 - mean_squared_error: 1.0004 - val_loss: 0.9896 - val_mean_squared_error: 0.9896\n",
      "Epoch 21/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9985 - mean_squared_error: 0.9985 - val_loss: 0.9848 - val_mean_squared_error: 0.9848\n",
      "Epoch 22/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: 0.9966 - mean_squared_error: 0.9966 - val_loss: 0.9786 - val_mean_squared_error: 0.9786\n",
      "Epoch 23/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9941 - mean_squared_error: 0.9941 - val_loss: 0.9767 - val_mean_squared_error: 0.9767\n",
      "Epoch 24/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9920 - mean_squared_error: 0.9920 - val_loss: 0.9782 - val_mean_squared_error: 0.9782\n",
      "Epoch 25/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9902 - mean_squared_error: 0.9902 - val_loss: 0.9778 - val_mean_squared_error: 0.9778\n",
      "Epoch 26/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9898 - mean_squared_error: 0.9898 - val_loss: 0.9783 - val_mean_squared_error: 0.9783\n",
      "Epoch 27/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9874 - mean_squared_error: 0.9874 - val_loss: 0.9745 - val_mean_squared_error: 0.9745\n",
      "Epoch 28/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9851 - mean_squared_error: 0.9851 - val_loss: 0.9726 - val_mean_squared_error: 0.9726\n",
      "Epoch 29/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9848 - mean_squared_error: 0.9848 - val_loss: 0.9711 - val_mean_squared_error: 0.9711\n",
      "Epoch 30/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9848 - mean_squared_error: 0.9848 - val_loss: 0.9672 - val_mean_squared_error: 0.9672\n",
      "Epoch 31/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 0.9818 - mean_squared_error: 0.9818 - val_loss: 0.9690 - val_mean_squared_error: 0.9690\n",
      "Epoch 32/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9805 - mean_squared_error: 0.9805 - val_loss: 0.9690 - val_mean_squared_error: 0.9690\n",
      "Epoch 33/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 0.9796 - mean_squared_error: 0.9796 - val_loss: 0.9676 - val_mean_squared_error: 0.9676\n",
      "Epoch 34/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 0.9783 - mean_squared_error: 0.9783 - val_loss: 0.9682 - val_mean_squared_error: 0.9682\n",
      "Epoch 35/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 0.9767 - mean_squared_error: 0.9767 - val_loss: 0.9671 - val_mean_squared_error: 0.9671\n",
      "Epoch 36/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9754 - mean_squared_error: 0.9754 - val_loss: 0.9647 - val_mean_squared_error: 0.9647\n",
      "Epoch 37/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9745 - mean_squared_error: 0.9745 - val_loss: 0.9642 - val_mean_squared_error: 0.9642\n",
      "Epoch 38/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 0.9735 - mean_squared_error: 0.9735 - val_loss: 0.9609 - val_mean_squared_error: 0.9609\n",
      "Epoch 39/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 0.9722 - mean_squared_error: 0.9722 - val_loss: 0.9591 - val_mean_squared_error: 0.9591\n",
      "Epoch 40/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9704 - mean_squared_error: 0.9704 - val_loss: 0.9559 - val_mean_squared_error: 0.9559\n",
      "Epoch 41/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 0.9697 - mean_squared_error: 0.9697 - val_loss: 0.9570 - val_mean_squared_error: 0.9570\n",
      "Epoch 42/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9686 - mean_squared_error: 0.9686 - val_loss: 0.9544 - val_mean_squared_error: 0.9544\n",
      "Epoch 43/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9679 - mean_squared_error: 0.9679 - val_loss: 0.9556 - val_mean_squared_error: 0.9556\n",
      "Epoch 44/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9673 - mean_squared_error: 0.9673 - val_loss: 0.9568 - val_mean_squared_error: 0.9568\n",
      "Epoch 45/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9656 - mean_squared_error: 0.9656 - val_loss: 0.9577 - val_mean_squared_error: 0.9577\n",
      "Epoch 46/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9651 - mean_squared_error: 0.9651 - val_loss: 0.9514 - val_mean_squared_error: 0.9514\n",
      "Epoch 47/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 0.9637 - mean_squared_error: 0.9637 - val_loss: 0.9512 - val_mean_squared_error: 0.9512\n",
      "Epoch 48/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 0.9623 - mean_squared_error: 0.9623 - val_loss: 0.9509 - val_mean_squared_error: 0.9509\n",
      "Epoch 49/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 0.9619 - mean_squared_error: 0.9619 - val_loss: 0.9515 - val_mean_squared_error: 0.9515\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s 65us/step - loss: 0.9612 - mean_squared_error: 0.9612 - val_loss: 0.9507 - val_mean_squared_error: 0.9507\n",
      "Epoch 51/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 0.9601 - mean_squared_error: 0.9601 - val_loss: 0.9491 - val_mean_squared_error: 0.9491\n",
      "Epoch 52/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 0.9589 - mean_squared_error: 0.9589 - val_loss: 0.9467 - val_mean_squared_error: 0.9467\n",
      "Epoch 53/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9584 - mean_squared_error: 0.9584 - val_loss: 0.9479 - val_mean_squared_error: 0.9479\n",
      "Epoch 54/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9581 - mean_squared_error: 0.9581 - val_loss: 0.9497 - val_mean_squared_error: 0.9497\n",
      "Epoch 55/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9563 - mean_squared_error: 0.9563 - val_loss: 0.9472 - val_mean_squared_error: 0.9472\n",
      "Epoch 56/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9561 - mean_squared_error: 0.9561 - val_loss: 0.9463 - val_mean_squared_error: 0.9463\n",
      "Epoch 57/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9551 - mean_squared_error: 0.9551 - val_loss: 0.9486 - val_mean_squared_error: 0.9486\n",
      "Epoch 58/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9547 - mean_squared_error: 0.9547 - val_loss: 0.9496 - val_mean_squared_error: 0.9496\n",
      "Epoch 59/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9551 - mean_squared_error: 0.9551 - val_loss: 0.9479 - val_mean_squared_error: 0.9479\n",
      "Epoch 60/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9534 - mean_squared_error: 0.9534 - val_loss: 0.9469 - val_mean_squared_error: 0.9469\n",
      "Epoch 61/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9526 - mean_squared_error: 0.9526 - val_loss: 0.9451 - val_mean_squared_error: 0.9451\n",
      "Epoch 62/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9516 - mean_squared_error: 0.9516 - val_loss: 0.9436 - val_mean_squared_error: 0.9436\n",
      "Epoch 63/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9512 - mean_squared_error: 0.9512 - val_loss: 0.9458 - val_mean_squared_error: 0.9458\n",
      "Epoch 64/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9502 - mean_squared_error: 0.9502 - val_loss: 0.9471 - val_mean_squared_error: 0.9471\n",
      "Epoch 65/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9495 - mean_squared_error: 0.9495 - val_loss: 0.9460 - val_mean_squared_error: 0.9460\n",
      "Epoch 66/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9503 - mean_squared_error: 0.9503 - val_loss: 0.9455 - val_mean_squared_error: 0.9455\n",
      "Epoch 67/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9481 - mean_squared_error: 0.9481 - val_loss: 0.9445 - val_mean_squared_error: 0.9445\n",
      "Epoch 68/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9482 - mean_squared_error: 0.9482 - val_loss: 0.9473 - val_mean_squared_error: 0.9473\n",
      "Epoch 69/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9470 - mean_squared_error: 0.9470 - val_loss: 0.9460 - val_mean_squared_error: 0.9460\n",
      "Epoch 70/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9467 - mean_squared_error: 0.9467 - val_loss: 0.9439 - val_mean_squared_error: 0.9439\n",
      "Epoch 71/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9458 - mean_squared_error: 0.9458 - val_loss: 0.9429 - val_mean_squared_error: 0.9429\n",
      "Epoch 72/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9441 - mean_squared_error: 0.9441 - val_loss: 0.9373 - val_mean_squared_error: 0.9373\n",
      "Epoch 73/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9439 - mean_squared_error: 0.9439 - val_loss: 0.9399 - val_mean_squared_error: 0.9399\n",
      "Epoch 74/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9431 - mean_squared_error: 0.9431 - val_loss: 0.9413 - val_mean_squared_error: 0.9413\n",
      "Epoch 75/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9431 - mean_squared_error: 0.9431 - val_loss: 0.9423 - val_mean_squared_error: 0.9423\n",
      "Epoch 76/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9409 - mean_squared_error: 0.9409 - val_loss: 0.9415 - val_mean_squared_error: 0.9415\n",
      "Epoch 77/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9412 - mean_squared_error: 0.9412 - val_loss: 0.9412 - val_mean_squared_error: 0.9412\n",
      "Epoch 78/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 0.9400 - mean_squared_error: 0.9400 - val_loss: 0.9402 - val_mean_squared_error: 0.9402\n",
      "Epoch 79/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9389 - mean_squared_error: 0.9389 - val_loss: 0.9410 - val_mean_squared_error: 0.9410\n",
      "Epoch 80/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 0.9388 - mean_squared_error: 0.9388 - val_loss: 0.9367 - val_mean_squared_error: 0.9367\n",
      "Epoch 81/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 0.9383 - mean_squared_error: 0.9383 - val_loss: 0.9390 - val_mean_squared_error: 0.9390\n",
      "Epoch 82/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9377 - mean_squared_error: 0.9377 - val_loss: 0.9401 - val_mean_squared_error: 0.9401\n",
      "Epoch 83/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9377 - mean_squared_error: 0.9377 - val_loss: 0.9388 - val_mean_squared_error: 0.9388\n",
      "Epoch 84/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 0.9362 - mean_squared_error: 0.9362 - val_loss: 0.9279 - val_mean_squared_error: 0.9279\n",
      "Epoch 85/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 0.9380 - mean_squared_error: 0.9380 - val_loss: 0.9306 - val_mean_squared_error: 0.9306\n",
      "Epoch 86/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 0.9349 - mean_squared_error: 0.9349 - val_loss: 0.9298 - val_mean_squared_error: 0.9298\n",
      "Epoch 87/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9352 - mean_squared_error: 0.9352 - val_loss: 0.9252 - val_mean_squared_error: 0.9252\n",
      "Epoch 88/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 0.9360 - mean_squared_error: 0.9360 - val_loss: 0.9292 - val_mean_squared_error: 0.9292\n",
      "Epoch 89/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9339 - mean_squared_error: 0.9339 - val_loss: 0.9338 - val_mean_squared_error: 0.9338\n",
      "Epoch 90/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9321 - mean_squared_error: 0.9321 - val_loss: 0.9371 - val_mean_squared_error: 0.9371\n",
      "Epoch 91/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9327 - mean_squared_error: 0.9327 - val_loss: 0.9411 - val_mean_squared_error: 0.9411\n",
      "Epoch 92/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9323 - mean_squared_error: 0.9323 - val_loss: 0.9393 - val_mean_squared_error: 0.9393\n",
      "Epoch 93/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 0.9334 - mean_squared_error: 0.9334 - val_loss: 0.9383 - val_mean_squared_error: 0.9383\n",
      "Epoch 94/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 0.9314 - mean_squared_error: 0.9314 - val_loss: 0.9393 - val_mean_squared_error: 0.9393\n",
      "Epoch 95/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 0.9308 - mean_squared_error: 0.9308 - val_loss: 0.9401 - val_mean_squared_error: 0.9401\n",
      "Epoch 96/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9312 - mean_squared_error: 0.9312 - val_loss: 0.9358 - val_mean_squared_error: 0.9358\n",
      "Epoch 97/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 0.9311 - mean_squared_error: 0.9311 - val_loss: 0.9346 - val_mean_squared_error: 0.9346\n",
      "Epoch 98/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9307 - mean_squared_error: 0.9307 - val_loss: 0.9362 - val_mean_squared_error: 0.9362\n",
      "Epoch 99/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 0.9309 - mean_squared_error: 0.9309 - val_loss: 0.9358 - val_mean_squared_error: 0.9358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 0.9304 - mean_squared_error: 0.9304 - val_loss: 0.9318 - val_mean_squared_error: 0.9318\n"
     ]
    }
   ],
   "source": [
    "#Your code here; rebuild a simple model using a relu layer followed by a linear layer. (See our code snippet above!)\n",
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"sgd\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's recheck our loss function. Not only should it be populated with numerical data as opposed to null values, but we also should expect to see the loss function decreasing with successive epochs, demonstrating optimization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.2408857441911794,\n",
       " 1.175496167907811,\n",
       " 1.1352448434841753,\n",
       " 1.1083863413695134,\n",
       " 1.0902548474494858,\n",
       " 1.0721652061499731,\n",
       " 1.0611876578945103,\n",
       " 1.0505221260316444,\n",
       " 1.042564938616271,\n",
       " 1.036092510127058]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history['loss'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have a converged model. With that, let's investigate how well the model performed with our good old friend, mean squarred error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE_train: 0.9279412804857353\n",
      "MSE_val: 0.9317675837282465\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)  \n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)\n",
    "\n",
    "print(\"MSE_train:\", MSE_train)\n",
    "print(\"MSE_val:\", MSE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Weight Initializers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and use a weight initializer. In the lecture, we've seen the He normalizer, which initializes the weight vector to have an average 0 and a variance of 2/n, with $n$ the number of features feeding into a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=12, kernel_initializer= \"he_normal\",\n",
    "                activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"sgd\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val),verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9266535553389985\n",
      "0.9473900233593091\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train)\n",
    "print(MSE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initializer does not really help us to decrease the MSE. We know that initializers can be particularly helpful in deeper networks, and our network isn't very deep. What if we use the `Lecun` initializer with a `tanh` activation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecun Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=12, \n",
    "                kernel_initializer= \"lecun_normal\", activation='tanh'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"sgd\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9274745950284212\n",
      "0.9463000994238832\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train)\n",
    "print(MSE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much of a difference, but a useful note to consider when tuning your network. Next, let's investigate the impace of various optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"rmsprop\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9144474237934009\n",
      "0.9436258157856549\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train)\n",
    "print(MSE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"Adam\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9113953333041319\n",
      "0.9446413240107423\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train)\n",
    "print(MSE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Decay with Momentum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "sgd = optimizers.SGD(lr=0.03, decay=0.0001, momentum=0.9)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= sgd ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8281274705237425\n",
      "0.9202899498812894\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train)\n",
    "print(MSE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb  \n",
    "\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database  \n",
    "\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/  \n",
    "\n",
    "* https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/  \n",
    "\n",
    "* https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/  \n",
    "\n",
    "* https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network  \n",
    "\n",
    "* https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lab, we began to practice some of the concepts regarding normalization and optimization for neural networks. In the final lab for this section, you'll independently practice these concepts on your own in order to tune a model to predict individuals payments to loans."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
